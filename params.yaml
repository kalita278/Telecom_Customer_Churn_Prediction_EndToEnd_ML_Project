DecisionTree:
  criterion: "entropy"
  max_depth: 5
  min_samples_leaf: 200

RandomForest:
  max_depth: 5
  max_features: 0.2
  max_samples: 0.3
  n_estimators: 200

Adaboost:
  algorithm: 'SAMME.R'
  learning_rate: 1
  n_estimators: 50

GradientBoost:
  loss: 'log_loss'
  learning_rate: 0.1
  n_estimators: 100
  subsample: 1.0
  criterion: 'friedman_mse'
  min_samples_split: 2
  min_samples_leaf: 1
  min_weight_fraction_leaf: 0.0
  max_depth: 3
  min_impurity_decrease: 0.0
  random_state: 43
  max_features: 0.3
  verbose: 0
  warm_start: False
  validation_fraction: 0.1
  tol: 0.0001
  ccp_alpha: 0.0


LogisticReg:
  penalty: 'l1'
  dual: False
  tol: 0.0001
  C: 10
  fit_intercept: True
  intercept_scaling: 1
  random_state: 43
  solver: 'saga'
  max_iter: 100
  multi_class: 'auto'
  verbose: 0
  warm_start: False

svc_clss:
  C: 100
  kernel: 'rbf'
  degree: 3
  gamma: 0.001
  coef0: 0.0
  shrinking: True
  tol: 0.001
  cache_size: 200
  verbose: False
  max_iter: -1
  decision_function_shape: 'ovr'
  break_ties: False
  random_state: 43
  probability: True

KNeighbors:
  n_neighbors: 13
  weights: 'uniform'
  algorithm: 'auto'
  leaf_size: 1
  p: 2
  metric: 'manhattan'

Naivebayes: